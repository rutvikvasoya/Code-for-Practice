{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "503266bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "There are 1 no of faces in the image training/modi/1.jpg\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "There are 1 no of faces in the image training/modi/2.jpg\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "There are 1 no of faces in the image training/modi/3.jpg\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "There are 1 no of faces in the image training/biden/1.jpg\n",
      "input shape of the model\n",
      "[<KerasTensor: shape=(None, 224, 224, 3) dtype=float32 (created by layer 'input_244')>]\n",
      "1/1 [==============================] - 0s 284ms/step\n",
      "********* cosine consider 0.5 as the threshold. if less then its a match*****\n",
      "0.161182701587677\n",
      "0.1285027265548706\n",
      "0.36431628465652466\n",
      "********* euclidean consider 100 as the threshold. if less then its a match*****\n",
      "74.43370819091797\n",
      "65.7414779663086\n",
      "109.36524963378906\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.python.keras.engine\n",
    "\n",
    "from keras_vggface.utils import preprocess_input\n",
    "import tensorflow.python.keras.engine\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from keras.utils.layer_utils import get_source_inputs\n",
    "import tensorflow.python.keras.engine\n",
    "\n",
    "\n",
    "def detect_extract_face(image_path_to_detect):\n",
    "    \n",
    "    #loading the image to detect\n",
    "    image_to_detect = plt.imread(image_path_to_detect)\n",
    "    \n",
    "    #create an instance of MTCNN detector\n",
    "    mtcnn_detector = MTCNN()\n",
    "    \n",
    "    #detect all face locations using the mtcnn dectector\n",
    "    all_face_locations = mtcnn_detector.detect_faces(image_to_detect)\n",
    "    \n",
    "    #print the number of faces detected\n",
    "    print('There are {} no of faces in the image {}'.format(len(all_face_locations),image_path_to_detect))\n",
    "    #print(all_face_locations)\n",
    "    \n",
    "    \n",
    "    #looping through the face locations\n",
    "    for index,current_face_location in enumerate(all_face_locations):\n",
    "        #splitting the tuple to get the four position values of current face\n",
    "        x,y,width,height = current_face_location['box']\n",
    "        #start co-ordinates\n",
    "        left_x, left_y = x,y\n",
    "        #end co-ordinates\n",
    "        right_x, right_y = x+width, y+height\n",
    "        #printing the location of current face\n",
    "        #print('Found face {} at left_x:{},left_y:{},right_x:{},right_y:{}'.format(index+1,left_x,left_y,right_x,right_y))\n",
    "        #slicing the current face from main image\n",
    "        current_face_image = image_to_detect[left_y:right_y,left_x:right_x]\n",
    "        #convert image from plt array to pil array\n",
    "        current_face_image = Image.fromarray(current_face_image)\n",
    "        #resize image to prefered size\n",
    "        current_face_image = current_face_image.resize((224,224))\n",
    "        #converting to numpy array\n",
    "        current_face_image_np_array = np.asarray(current_face_image)\n",
    "        #return array\n",
    "        return current_face_image_np_array\n",
    "    \n",
    "    \n",
    "        \n",
    "# #collecting the array of faces into a single list\n",
    "# sample_faces = [detect_extract_face('dataset/training/modi/1.jpg'),\n",
    "#                 detect_extract_face('dataset/training/modi/2.jpg'),\n",
    "#                 detect_extract_face('dataset/training/modi/3.jpg'),\n",
    "#                 detect_extract_face('dataset/training/biden/1.jpg')]  \n",
    "#collecting the array of faces into a single list\n",
    "sample_faces = [detect_extract_face('training/modi/1.jpg'),\n",
    "                detect_extract_face('training/modi/2.jpg'),\n",
    "                detect_extract_face('training/modi/3.jpg'),\n",
    "                detect_extract_face('training/biden/1.jpg')\n",
    "               ]\n",
    "        \n",
    "#convert to float 32 array\n",
    "sample_faces = np.asarray(sample_faces,'float32')\n",
    "#preprocess the array \n",
    "sample_faces = preprocess_input(sample_faces, version=2)\n",
    "#create the vgg face model\n",
    "vggface_model = VGGFace(include_top=False, model='vgg16', input_shape=(224,224,3), pooling='avg')\n",
    "\n",
    "\n",
    "print(\"input shape of the model\")\n",
    "print(vggface_model.inputs)      \n",
    "        \n",
    "#face feature (embeddings) extraction\n",
    "sample_faces_embeddings = vggface_model.predict(sample_faces)\n",
    "\n",
    "#the face that need to be verified\n",
    "modi_face_1 = sample_faces_embeddings[0]\n",
    "\n",
    "#the faces that will be verfied against\n",
    "modi_face_2 = sample_faces_embeddings[1]\n",
    "modi_face_3 = sample_faces_embeddings[2]\n",
    "biden_face_1 = sample_faces_embeddings[3]\n",
    "\n",
    "# Verify against the known photographs using cosine distance\n",
    "print('********* cosine consider 0.5 as the threshold. if less then its a match*****')\n",
    "print(cosine(modi_face_1, modi_face_2))\n",
    "print(cosine(modi_face_1, modi_face_3))\n",
    "print(cosine(modi_face_1, biden_face_1))\n",
    "\n",
    "# Verify against the known photographs using euclidean distance\n",
    "print('********* euclidean consider 100 as the threshold. if less then its a match*****')\n",
    "print(euclidean(modi_face_1, modi_face_2))\n",
    "print(euclidean(modi_face_1, modi_face_3))\n",
    "print(euclidean(modi_face_1, biden_face_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7377772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
